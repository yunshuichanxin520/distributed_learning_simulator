---
dataset_name: imdb
model_name: TransformerClassificationModel
distributed_algorithm: com_fed_shapley_value
optimizer_name: SGD
worker_number: 6
batch_size: 64
round: 100
learning_rate_scheduler_name: CosineAnnealingLR
epoch: 5
learning_rate: 0.01
dataset_kwargs:
  input_max_len: 300
  dataset_type: text
  tokenizer:
    type: spacy
model_kwargs:
  word_vector_name: glove.6B.100d
  num_encoder_layer: 2
  d_model: 100
  nhead: 5

algorithm_kwargs:
  random_client_number: 3
dataset_sampling: iid

#dataset_sampling: iid_split_and_sample
#dataset_sampling_kwargs:
#   sample_probs:
#      - {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}
#      - {0: 0.1, 1: 0.1, 2: 0.1, 3: 0.1, 4: 0.1, 5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1}
#      - {0: 0.5, 1: 0.5, 2: 0.5, 3: 0.5, 4: 0.5, 5: 0.5, 6: 0.5, 7: 0.5, 8: 0.5, 9: 0.5}
#      - {0: 0.5, 1: 0.5, 2: 0.5, 3: 0.5, 4: 0.5, 5: 0.5, 6: 0.5, 7: 0.5, 8: 0.5, 9: 0.5}
#      - {0: 0.6, 1: 0.6, 2: 0.6, 3: 0.6, 4: 0.6, 5: 0.6, 6: 0.6, 7: 0.6, 8: 0.6, 9: 0.6}
#      - {0: 0.6, 1: 0.6, 2: 0.6, 3: 0.6, 4: 0.6, 5: 0.6, 6: 0.6, 7: 0.6, 8: 0.6, 9: 0.6}


#dataset_sampling: iid_split_and_flip
#dataset_sampling_kwargs:
#  flip_percent:
#    - 0.1
#    - 0.1
#    - 0.5
#    - 0.5
#    - 0.9
#    - 0.9

#dataset_sampling: dirichlet_split
#dataset_sampling_kwargs:
#  concentration: 0.5